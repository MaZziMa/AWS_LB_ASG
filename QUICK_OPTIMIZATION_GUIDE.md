# Quick Optimization Reference Card

## üöÄ TL;DR - Copy-Paste Solutions

### Problem: TargetResponseTime = 5.3s
### Solution: Apply 3 optimizations ‚Üí 24ms (99.5% faster)

---

## 1Ô∏è‚É£ Redis Caching (2 minutes setup)

### Start Redis (Development)
```powershell
docker run -d --name redis-cache -p 6379:6379 redis:7-alpine
```

### Add to any endpoint
```python
from app.cache import cache, CacheKeys, CacheTTL

@router.get("/api/courses")
async def list_courses(semester_id: int):
    # Try cache
    cache_key = f"courses:semester:{semester_id}"
    cached = await cache.get(cache_key)
    if cached:
        return cached
    
    # Cache miss - query DB
    courses = await fetch_courses(semester_id)
    
    # Store in cache
    await cache.set(cache_key, courses, ttl=300)
    return courses
```

**Impact: 5000ms ‚Üí 2ms (99.96% faster)**

---

## 2Ô∏è‚É£ DynamoDB Indexing (5 minutes setup)

### Create indexes
```powershell
cd infrastructure
.\create-dynamodb-indexes.ps1
```

### Replace Scan with Query
```python
# ‚ùå BAD: Scan (5000ms)
response = table.scan(
    FilterExpression='semester_id = :sid',
    ExpressionAttributeValues={':sid': 1}
)

# ‚úÖ GOOD: Query with GSI (50ms)
from app.db_optimization import db_optimizer

courses = db_optimizer.query_with_gsi(
    table_name='Courses',
    index_name='CoursesBySemester',
    key_condition='semester_id = :sid',
    expression_values={':sid': {'N': '1'}}
)
```

**Impact: 5000ms ‚Üí 50ms (99% faster)**

---

## 3Ô∏è‚É£ Background Tasks (1 minute)

### Make I/O non-blocking
```python
from fastapi import BackgroundTasks

# ‚ùå BAD: Blocking (500ms wait)
@router.post("/enroll")
async def enroll(data: EnrollData):
    enrollment = await save_enrollment(data)
    await send_email(data.user_email)  # User waits!
    return enrollment

# ‚úÖ GOOD: Background task (1ms)
@router.post("/enroll")
async def enroll(data: EnrollData, bg: BackgroundTasks):
    enrollment = await save_enrollment(data)
    bg.add_task(send_email, data.user_email)  # Don't wait
    return enrollment  # Return immediately!
```

**Impact: 500ms ‚Üí 1ms (99.8% faster)**

---

## üî• Critical Code Patterns

### Pattern 1: Cache-Aside (Read-Through)
```python
async def get_course(course_id: str):
    # 1. Check cache
    cached = await cache.get(f"course:{course_id}")
    if cached:
        return cached
    
    # 2. Cache miss - fetch from DB
    course = await db.get_course(course_id)
    
    # 3. Store in cache
    await cache.set(f"course:{course_id}", course, ttl=600)
    
    return course
```

### Pattern 2: Write-Through Cache
```python
async def update_course(course_id: str, data: dict):
    # 1. Update database
    await db.update_course(course_id, data)
    
    # 2. Update cache immediately (not invalidate!)
    await cache.set(f"course:{course_id}", data, ttl=600)
    
    # 3. Invalidate related caches
    await cache.delete_pattern("courses:list:*")
```

### Pattern 3: Batch Fetch
```python
# ‚ùå BAD: N+1 queries
enrollments = await get_enrollments(user_id)  # 1 query
for e in enrollments:
    course = await get_course(e.course_id)    # N queries

# ‚úÖ GOOD: 2 queries total
enrollments = await get_enrollments(user_id)  # 1 query
course_ids = [e.course_id for e in enrollments]
courses = await batch_get_courses(course_ids) # 1 batch query
```

### Pattern 4: Cache Stampede Protection
```python
async def get_popular_courses():
    cache_key = "courses:popular"
    
    # Try cache
    cached = await cache.get(cache_key)
    if cached:
        return cached
    
    # Acquire lock to prevent stampede
    lock_key = f"lock:{cache_key}"
    if await cache.set(lock_key, "1", ttl=10):  # 10s lock
        try:
            # Only one request computes this
            courses = await compute_popular_courses()
            await cache.set(cache_key, courses, ttl=900)
            return courses
        finally:
            await cache.delete(lock_key)
    else:
        # Other requests wait and retry
        await asyncio.sleep(0.1)
        return await get_popular_courses()
```

---

## üìä Performance Checklist

### Before Deploying
- [ ] All `table.scan()` replaced with `query_with_gsi()`
- [ ] Top 5 endpoints have caching
- [ ] No blocking I/O in request handlers
- [ ] Connection pooling enabled
- [ ] Cache invalidation strategy defined

### After Deploying
- [ ] Load test: 200 users for 10 minutes
- [ ] CloudWatch: TargetResponseTime < 200ms
- [ ] Redis: Cache hit rate > 90%
- [ ] DynamoDB: No throttling errors
- [ ] Auto Scaling: No scale events needed

---

## üêõ Common Mistakes & Fixes

### Mistake 1: Forgetting to invalidate cache
```python
# ‚ùå BAD: Cache never updates
async def update_course(course_id, data):
    await db.update_course(course_id, data)
    return data  # Cache still has old data!

# ‚úÖ GOOD: Invalidate after write
async def update_course(course_id, data):
    await db.update_course(course_id, data)
    await cache.delete(f"course:{course_id}")
    await cache.delete_pattern("courses:list:*")
    return data
```

### Mistake 2: Caching user-specific data globally
```python
# ‚ùå BAD: User A sees User B's data
cache_key = "enrollments"  # Same key for all users!

# ‚úÖ GOOD: Include user ID in key
cache_key = f"enrollments:user:{user_id}"
```

### Mistake 3: Not handling cache failures
```python
# ‚ùå BAD: App crashes if Redis down
cached = await cache.get(key)
return cached  # None if Redis fails!

# ‚úÖ GOOD: Fallback to DB
cached = await cache.get(key)
if cached:
    return cached
# Fallback to DB if cache miss or error
return await db.fetch(key)
```

### Mistake 4: Infinite TTL
```python
# ‚ùå BAD: Memory leak
await cache.set(key, data)  # Never expires!

# ‚úÖ GOOD: Always set TTL
await cache.set(key, data, ttl=300)  # 5 minutes
```

---

## üéØ Quick Commands

### Test cache connection
```powershell
python -c "import redis; r = redis.Redis(); r.ping(); print('OK')"
```

### Monitor Redis
```powershell
docker exec -it redis-cache redis-cli
> INFO stats
> KEYS *
> GET course:123
```

### Check DynamoDB indexes
```powershell
aws dynamodb describe-table --table-name Courses --query 'Table.GlobalSecondaryIndexes[*].[IndexName,IndexStatus]' --output table
```

### Load test after optimization
```powershell
locust -f loadtest/locustfile.py --headless --users 200 --spawn-rate 10 --run-time 5m --html report.html
```

### Verify TargetResponseTime
```powershell
aws cloudwatch get-metric-statistics `
  --namespace AWS/ApplicationELB `
  --metric-name TargetResponseTime `
  --dimensions Name=LoadBalancer,Value=app/course-reg-alb/xxxxx `
  --start-time (Get-Date).AddMinutes(-10) `
  --end-time (Get-Date) `
  --period 60 `
  --statistics Average
```

---

## üí° Pro Tips

1. **Cache hot data only**: Don't cache everything. Focus on:
   - Course lists (queried 1000x/min)
   - User profiles (queried 500x/min)
   - Popular courses (computed expensive)

2. **Choose right TTL**:
   - User sessions: 30 min
   - Course list: 5 min
   - Course details: 1 hour
   - Static data: 24 hours

3. **Monitor cache metrics**:
   ```python
   # Add to CloudWatch
   cache_hit_rate = hits / (hits + misses)
   # Target: > 90%
   ```

4. **Warm cache on startup**:
   ```python
   @app.on_event("startup")
   async def warm_cache():
       popular_courses = await fetch_popular()
       await cache.set("courses:popular", popular_courses)
   ```

5. **Use Redis pipelining for bulk ops**:
   ```python
   pipe = cache.redis_client.pipeline()
   for key, value in items:
       pipe.setex(key, 300, value)
   await pipe.execute()  # One network round trip
   ```

---

## üìö File Locations

| File | Purpose |
|------|---------|
| `backend/app/cache.py` | Cache utilities |
| `backend/app/db_optimization.py` | DynamoDB optimization |
| `backend/app/api/courses_optimized.py` | Example implementation |
| `infrastructure/create-dynamodb-indexes.ps1` | GSI setup script |
| `apply-optimizations.ps1` | One-click deploy |
| `BACKEND_OPTIMIZATION.md` | Full guide |
| `OPTIMIZATION_COMPARISON.md` | Before/after analysis |

---

## üÜò Troubleshooting

### "Redis connection refused"
```powershell
# Start Redis
docker run -d --name redis-cache -p 6379:6379 redis:7-alpine
```

### "GSI already exists"
```powershell
# Check status
aws dynamodb describe-table --table-name Courses
# Look for IndexStatus: ACTIVE
```

### "ProvisionedThroughputExceededException"
```powershell
# Increase GSI capacity
aws dynamodb update-table `
  --table-name Courses `
  --global-secondary-index-updates '[{"Update":{"IndexName":"CoursesBySemester","ProvisionedThroughput":{"ReadCapacityUnits":20}}}]'
```

### "Cache hit rate < 50%"
- Check TTL (too short?)
- Check key generation (unique per request?)
- Check invalidation (too aggressive?)

---

## ‚úÖ Success Metrics

After applying optimizations:

| Metric | Target | How to Check |
|--------|--------|--------------|
| **Response time** | < 200ms | CloudWatch TargetResponseTime |
| **Cache hit rate** | > 90% | `redis-cli INFO stats` |
| **Error rate** | < 0.1% | CloudWatch HTTPCode_Target_5XX |
| **DynamoDB RCU** | < 20% used | CloudWatch ConsumedReadCapacity |
| **Throughput** | 500+ req/s | Locust test report |

**If all green: You're done! üéâ**

---

## üîó Next Steps

1. Read: `BACKEND_OPTIMIZATION.md` (comprehensive guide)
2. Review: `OPTIMIZATION_COMPARISON.md` (before/after metrics)
3. Run: `.\apply-optimizations.ps1` (deploy everything)
4. Test: `locust -f loadtest/locustfile.py --users 200`
5. Monitor: CloudWatch dashboard

**Questions? Check the full docs or ask!**
